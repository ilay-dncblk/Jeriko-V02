{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv('ımdb_reviews_train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df=pd.read_csv('ımdb_reviews_valid.csv')\n",
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df=pd.read_csv('ımdb_reviews_test_unlabeled.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([train_df,valid_df],sort=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"veri seti boyutu:\",df.shape)\n",
    "print(\"###############################################\")\n",
    "print(df.info())\n",
    "print(\"###############################################\")\n",
    "print(df['sentiment'].value_counts())\n",
    "print(\"###############################################\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'sentiment' sütunundaki değerleri düzelt\n",
    "df['sentiment'] = df['sentiment'].replace({\n",
    "    '1;': 'positive',\n",
    "    '0;': 'negative',\n",
    "    'positive\"': 'positive',\n",
    "    'negative;;': 'negative',\n",
    "    'positive;;': 'positive',\n",
    "    '0\"': 'negative',\n",
    "})\n",
    "\n",
    "\n",
    "# Sonuçları kontrol et\n",
    "print(df['sentiment'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # 1.HTML etiketlerini kaldırma\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # 2.URL'leri kaldırma\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # 3.Gereksiz boşlukları kaldırma\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # 4. Küçük harflere dönüştür\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 5. Noktalama işaretlerini kaldır\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(clean_text)\n",
    "print(df['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import nltk\n",
    "# NLTK verilerini belirli bir dizine indir\n",
    "nltk.data.path.append('/GANsAugmentation-SentimentAnalysis/nltk_data')  \n",
    "\n",
    "# Gerekli NLTK bileşenlerini indir\n",
    "nltk.download('all', download_dir='/GANsAugmentation-SentimentAnalysis/nltk_data')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "def remove_repeated_phrases(text):\n",
    "    # Metni kelimelere böl\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Her kelimenin kaç kez tekrar ettiğini say\n",
    "    word_freq = Counter(words)\n",
    "    print(word_freq)\n",
    "    \n",
    "    # Sadece bir kez geçen kelimeleri al\n",
    "    non_repeated_words = [word for word in words if word_freq[word] == 1]\n",
    "    \n",
    "    # Bu kelimeleri birleştirip metni oluştur\n",
    "    cleaned_text = ' '.join(non_repeated_words)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Örnek kullanım\n",
    "sample_text = \"This is a test. This is only a test.\"\n",
    "cleaned_text = remove_repeated_phrases(sample_text)\n",
    "print(cleaned_text)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(remove_repeated_phrases)\n",
    "print(df['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from textblob import TextBlob\n",
    "import multiprocessing as mp\n",
    "\n",
    "def correct_spelling(text):\n",
    "    text_blob = TextBlob(text)\n",
    "    return str(text_blob.correct())\n",
    "\n",
    "def process_in_parallel(data, num_workers):\n",
    "    with mp.Pool(num_workers) as pool:\n",
    "        results = pool.map(correct_spelling, data)\n",
    "    return results\n",
    "\n",
    "# Örnek kullanım\n",
    "data = [\"Your text data here\" ]\n",
    "num_workers = mp.cpu_count()  # İşlemcinizin çekirdek sayısına göre belirleyin\n",
    "corrected_data = process_in_parallel(data, num_workers)\n",
    "print(corrected_data)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from symspellpy import SymSpell, Verbosity\n",
    "\n",
    "# SymSpell örneği oluştur\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "\n",
    "# Sözlük yükleme (indirdiğiniz dosyanın tam yolunu belirtin)\n",
    "sym_spell.load_dictionary(\"frequency_dictionary_en_82_765.txt\", 0, 1)\n",
    "\n",
    "def correct_spelling_symspell(text):\n",
    "    corrected_text = []\n",
    "    for word in text.split():\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)\n",
    "        if suggestions:\n",
    "            corrected_text.append(suggestions[0].term)\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(correct_spelling_symspell)\n",
    "print(df['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from gensim.models import KeyedVectors\n",
    "#Synonym Replacement (Sinonim Değiştirme)\n",
    "#Bir kelimenin anlamını koruyarak metindeki kelimeleri değiştirme\n",
    "\n",
    "# Google'ın pre-trained Word2Vec modelini indirip yükleyin\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "def get_similar_words(word):\n",
    "    if word in model:\n",
    "        return [similar_word for similar_word, _ in model.most_similar(word)]\n",
    "    return [word]\n",
    "\n",
    "def replace_with_similar_words(text):\n",
    "    words = text.split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        similar_words = get_similar_words(word)\n",
    "        if similar_words:\n",
    "            new_words.append(similar_words[0])  # Replace with first similar word\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "# Test et\n",
    "#sentence = \"This movie was horrible. If it had never been made the world would be a better place. Come on, a flying wagon? What were they thinking? This was a sub-par movie with a horrible hook, and I would like a written apology from the studio that produced this, along with some cookies to help repay me for the time I wasted on this crap fest that I can never get back. If you payed to see this movie, I am truly sorry because I watched it on TV on a Sunday afternoon when I had nothing better to do and it pretty much ruined my whole week. A flying freaking WAGON?!?! And that's supposed to make up for having a horrible mother who cares more about her own screwed up needs than her children? No wonder they don't have enough sense to tell someone he is beating them, their mother teaches them nothing but that what she wants comes before everything else. Absolutely horrible.\"\n",
    "#sentence1=clean_text(sentence)\n",
    "#sentences2=remove_repeated_phrases(sentence1)\n",
    "#sentences3=correct_spelling_symspell(sentences2)\n",
    "#print(replace_with_similar_words(sentences3))\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def get_similar_words(word, threshold=0.6):\n",
    "    if word in model:\n",
    "        similar_words = model.most_similar(word)\n",
    "        # Sadece eşiğin üzerinde olan kelimeleri al\n",
    "        return [similar_word for similar_word, similarity in similar_words if similarity >= threshold]\n",
    "    return [word]\n",
    "\n",
    "def replace_with_similar_words(text):\n",
    "    words = text.split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        similar_words = get_similar_words(word)\n",
    "        if similar_words:\n",
    "            new_words.append(similar_words[0])  # Replace with first similar word\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "# Test et\n",
    "sentence = \"This movie was horrible. If it had never been made the world would be a better place. Come on, a flying wagon? What were they thinking? This was a sub-par movie with a horrible hook, and I would like a written apology from the studio that produced this, along with some cookies to help repay me for the time I wasted on this crap fest that I can never get back. If you payed to see this movie, I am truly sorry because I watched it on TV on a Sunday afternoon when I had nothing better to do and it pretty much ruined my whole week. A flying freaking WAGON?!?! And that's supposed to make up for having a horrible mother who cares more about her own screwed up needs than her children? No wonder they don't have enough sense to tell someone he is beating them, their mother teaches them nothing but that what she wants comes before everything else. Absolutely horrible.\"\n",
    "sentence1 = clean_text(sentence)\n",
    "sentences2 = remove_repeated_phrases(sentence1)\n",
    "sentences3 = correct_spelling_symspell(sentences2)\n",
    "print(replace_with_similar_words(sentences3))\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from transformers import pipeline\n",
    "\n",
    "# T5 modelini yükle\n",
    "paraphrase_model = pipeline(\"text2text-generation\", model=\"t5-base\")\n",
    "\n",
    "# Parafraz işlemi yap\n",
    "def paraphrase_text(text):\n",
    "    result = paraphrase_model(f\"paraphrase: {text}\", max_length=256, num_beams=5, early_stopping=True)\n",
    "    return result[0]['generated_text']\n",
    "\n",
    "# Örnek cümle\n",
    "sentence = \"This movie was horrible. If it had never been made the world would be a better place.\"\n",
    "paraphrased_sentence = paraphrase_text(sentence)\n",
    "print(paraphrased_sentence)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import random\n",
    "#noise Insertion (gürültü Ekleme)\n",
    "#Metne rastgele kelimeler ekleyerek metni bozma\n",
    "\n",
    "def add_noise(text):\n",
    "    words = text.split()\n",
    "    num_words_to_add = int(len(words) * 0.1)\n",
    "    for _ in range(num_words_to_add):\n",
    "        random_index = random.randint(0, len(words)-1)\n",
    "        words.insert(random_index, words[random_index])\n",
    "    return ' '.join(words)\n",
    "\n",
    "augmented_review = 'The movie was good and I liked it'\n",
    "# Örnek kullanımı\n",
    "noisy_review = add_noise(augmented_review)\n",
    "print(noisy_review)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def add_noise(text, noise_words):\n",
    "    words = text.split()\n",
    "    num_words_to_add = int(len(words) * 0.1)  # Metnin %10'u kadar gürültü kelimesi ekle\n",
    "    for _ in range(num_words_to_add):\n",
    "        random_index = random.randint(0, len(words) - 1)\n",
    "        noise_word = random.choice(noise_words)\n",
    "        words.insert(random_index, noise_word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Gürültü için kullanılacak kelimeler listesi\n",
    "noise_words = [\n",
    "    'um', 'uh', 'like', 'so', 'you know', 'actually', 'basically',\n",
    "    'seriously', 'literally', 'well', 'honestly', 'truly', 'really',\n",
    "    'I mean', 'basically', 'just', 'kinda', 'sorta', 'probably',\n",
    "    'maybe', 'definitely', 'literally', 'figuratively', 'almost'\n",
    "]\n",
    "\n",
    "# Örnek metin\n",
    "#augmented_review = 'her acting was good and I liked it'\n",
    "#noisy_review = add_noise(augmented_review, noise_words)\n",
    "#print(noisy_review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(add_noise, noise_words=noise_words)\n",
    "print(df['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Generator Ağı\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Discriminator Ağı\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Model Boyutları\n",
    "input_size = 100\n",
    "hidden_size = 50\n",
    "output_size = 100\n",
    "\n",
    "# Modelleri Tanımla\n",
    "generator = Generator(input_size, hidden_size, output_size)\n",
    "discriminator = Discriminator(output_size, hidden_size)\n",
    "\n",
    "# Kayıp Fonksiyonları ve Optimizatörler\n",
    "criterion = nn.BCELoss()\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.001)\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.001)\n",
    "\n",
    "# Eğitim döngüsü (basit örnek)\n",
    "for epoch in range(10000):\n",
    "    # Gerçek ve sahte cümleler için etiketler\n",
    "    real_labels = torch.ones(1, 1)\n",
    "    fake_labels = torch.zeros(1, 1)\n",
    "\n",
    "    # Gerçek verilerden (cümlelerden) örnek al\n",
    "    real_data = torch.randn(1, output_size)  # Bu, gerçek veri yerine rastgele bir placeholder\n",
    "    fake_data = generator(torch.randn(1, input_size))\n",
    "\n",
    "    # Discriminator'u eğit\n",
    "    d_optimizer.zero_grad()\n",
    "    real_output = discriminator(real_data)\n",
    "    fake_output = discriminator(fake_data.detach())\n",
    "    d_loss_real = criterion(real_output, real_labels)\n",
    "    d_loss_fake = criterion(fake_output, fake_labels)\n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "    d_loss.backward()\n",
    "    d_optimizer.step()\n",
    "\n",
    "    # Generator'u eğit\n",
    "    g_optimizer.zero_grad()\n",
    "    fake_output = discriminator(fake_data)\n",
    "    g_loss = criterion(fake_output, real_labels)\n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Epoch {epoch}, D Loss: {d_loss.item()}, G Loss: {g_loss.item()}')\n",
    "\n",
    "# Eğitim tamamlandığında cümleleri paraphrase et\n",
    "new_sentence = generator(torch.randn(1, input_size))\n",
    "print(\"Generated Sentence Paraphrase:\", new_sentence)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['review'] = df['review'].apply(lambda x: generator(torch.randn(1, input_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
