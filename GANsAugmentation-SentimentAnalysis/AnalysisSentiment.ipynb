{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_df=pd.read_csv('ımdb_reviews_train.csv')\n",
    "valid_df=pd.read_csv('ımdb_reviews_valid.csv')\n",
    "df=pd.concat([train_df,valid_df],sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # 1.HTML etiketlerini kaldırma\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # 2.URL'leri kaldırma\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # 3.Gereksiz boşlukları kaldırma\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Başlangıçtaki ve sondaki boşlukları temizle\n",
    "    text = text.strip()\n",
    "    # Cümle içindeki fazla boşlukları temizle\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # 4. Küçük harflere dönüştür\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 5. Noktalama işaretlerini kaldır\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(lambda x: clean_text(x))\n",
    "print(df['review'].head())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Yinelenen kelimeleri temizleme fonksiyonu\n",
    "def remove_duplicate_words(text):\n",
    "    words = text.split()\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for word in words:\n",
    "        if word.lower() not in seen:  # Küçük harf duyarlılığı ile kontrol ediyoruz\n",
    "            seen.add(word.lower())\n",
    "            result.append(word)\n",
    "    return \" \".join(result)\n",
    "\n",
    "# Yinelenen cümleleri temizleme fonksiyonu\n",
    "def remove_duplicate_sentences(text):\n",
    "    sentences = text.split('.')\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if sentence.lower() not in seen:\n",
    "            seen.add(sentence.lower())\n",
    "            result.append(sentence)\n",
    "    return \". \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yinelenen kelimeleri ve cümleleri temizleyelim\n",
    "df['review'] = df['review'].apply(remove_duplicate_words)\n",
    "print(df['review'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import pandas as pd\n",
    "from autocorrect import Speller\n",
    "\n",
    "# Speller nesnesi oluşturma\n",
    "spell = Speller(lang='en')\n",
    "\n",
    "# Yazım hatalarını düzeltme fonksiyonu\n",
    "def correct_spelling_autocorrect(text):\n",
    "    return spell(text)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tüm veride yazım hatalarını düzeltelim\n",
    "df['review'] = df['review'].apply(correct_spelling_autocorrect)\n",
    "print(df['review'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import language_tool_python\n",
    "\n",
    "# LanguageTool aracı\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "# Gramer hatalarını düzeltme fonksiyonu\n",
    "def correct_grammar(text):\n",
    "    matches = tool.check(text)\n",
    "    corrected_text = language_tool_python.utils.correct(text, matches)\n",
    "    return corrected_text\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(correct_grammar)\n",
    "print(df['review'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Gerekli NLTK paketlerini indirme\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Kelimenin sinonimini bulma fonksiyonu\n",
    "def get_synonym(word):\n",
    "    synonyms = wordnet.synsets(word)\n",
    "    if synonyms:\n",
    "        synonym = synonyms[0].lemmas()[0].name()\n",
    "        return synonym if synonym != word else None\n",
    "    return None\n",
    "\n",
    "# Cümledeki kelimeleri sinonimleri ile değiştirme fonksiyonu\n",
    "def replace_with_synonyms(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    pos_tagged = nltk.pos_tag(words)\n",
    "    new_words = []\n",
    "    \n",
    "    for word, tag in pos_tagged:\n",
    "        # Yalnızca isimler, fiiller, sıfatlar ve zarflar üzerinde değişiklik yapalım\n",
    "        if tag.startswith('NN') or tag.startswith('VB') or tag.startswith('JJ') or tag.startswith('RB'):\n",
    "            synonym = get_synonym(word)\n",
    "            if synonym:\n",
    "                new_words.append(synonym)\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return ' '.join(new_words)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verideki tüm cümlelerde sinonim değiştirme\n",
    "df['review'] = df['review'].apply(replace_with_synonyms)\n",
    "print(df['review'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "\n",
    "# NLTK veri setlerini indirme\n",
    "nltk.download('punkt')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Dataset Sınıfı\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, reviews):\n",
    "        self.reviews = reviews\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.reviews[idx]\n",
    "\n",
    "# Generator Ağı\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Discriminator Ağı\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Hiperparametreler\n",
    "input_size = 100  # Rastgele gürültü vektör boyutu\n",
    "hidden_size = 128  # Gizli katman boyutu\n",
    "output_size = 200  # Üretilecek cümle vektör boyutu\n",
    "batch_size = 32\n",
    "learning_rate = 0.0002\n",
    "num_epochs = 1000\n",
    "\n",
    "# Model örnekleri\n",
    "generator = Generator(input_size, hidden_size, output_size)\n",
    "discriminator = Discriminator(output_size, hidden_size)\n",
    "\n",
    "# Optimizasyon\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "# Kayıp fonksiyonu\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Veriyi yükleme\n",
    "reviews = df['review'].tolist()\n",
    "dataset = ReviewDataset(reviews)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Eğitim döngüsü\n",
    "for epoch in range(num_epochs):\n",
    "    for i, real_data in enumerate(dataloader):\n",
    "        batch_size = real_data.size(0)\n",
    "\n",
    "        # Discriminator için gerçek ve sahte etiketler\n",
    "        real_labels = torch.ones(batch_size, 1)\n",
    "        fake_labels = torch.zeros(batch_size, 1)\n",
    "\n",
    "        # Discriminator'ı eğitme\n",
    "        optimizer_d.zero_grad()\n",
    "        real_data = real_data.float()\n",
    "        real_output = discriminator(real_data)\n",
    "        real_loss = criterion(real_output, real_labels)\n",
    "        real_loss.backward()\n",
    "\n",
    "        noise = torch.randn(batch_size, input_size)\n",
    "        fake_data = generator(noise)\n",
    "        fake_output = discriminator(fake_data.detach())\n",
    "        fake_loss = criterion(fake_output, fake_labels)\n",
    "        fake_loss.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # Generator'ı eğitme\n",
    "        optimizer_g.zero_grad()\n",
    "        fake_output = discriminator(fake_data)\n",
    "        generator_loss = criterion(fake_output, real_labels)\n",
    "        generator_loss.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], D Loss: {real_loss.item()+fake_loss.item()}, G Loss: {generator_loss.item()}')\n",
    "\n",
    "# Üretilmiş cümleleri kaydetme\n",
    "generated_reviews = []\n",
    "for i in range(1000):  # 1000 tane cümle üretelim\n",
    "    noise = torch.randn(1, input_size)\n",
    "    generated_review = generator(noise)\n",
    "    generated_reviews.append(generated_review)\n",
    "\n",
    "# Üretilen cümleleri CSV dosyasına kaydetme\n",
    "generated_df = pd.DataFrame({'review': generated_reviews})\n",
    "generated_df.to_csv('generated_reviews.csv', index=False)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import random\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Gürültü ekleme fonksiyonu\n",
    "def inject_noise(text, noise_level=0.05):\n",
    "    words = word_tokenize(text)\n",
    "    num_words = len(words)\n",
    "    noise_words = int(num_words * noise_level)\n",
    "    \n",
    "    # Rastgele yerlerden kelimeler ekleyelim\n",
    "    for _ in range(noise_words):\n",
    "        position = random.randint(0, num_words - 1)\n",
    "        noise_word = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz', k=3))  # 3 harfli rastgele kelime\n",
    "        words.insert(position, noise_word)\n",
    "    \n",
    "    return ' '.join(words)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gürültü ekleyelim\n",
    "df['review'] = df['review'].apply(inject_noise)\n",
    "print(df['review'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Duygusal tonları temsil eden sınıflar\n",
    "emotions = ['happy', 'angry', 'sad', 'neutral']\n",
    "\n",
    "# Geliştirilmiş Generator\n",
    "class EmotionGenerator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(EmotionGenerator, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.fc3 = nn.Linear(output_size, len(emotions))  # Duygusal tonlar için çıktı katmanı\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Geliştirilmiş Discriminator\n",
    "class EmotionDiscriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EmotionDiscriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, len(emotions))  # Duygusal tonlar için çıktı katmanı\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Hiperparametreler\n",
    "input_size = 200  # Girdi boyutu\n",
    "hidden_size = 128  # Gizli katman boyutu\n",
    "output_size = 200  # Çıktı boyutu\n",
    "batch_size = 32\n",
    "learning_rate = 0.0002\n",
    "num_epochs = 1000\n",
    "\n",
    "# Model örnekleri\n",
    "generator = EmotionGenerator(input_size, hidden_size, output_size)\n",
    "discriminator = EmotionDiscriminator(output_size, hidden_size)\n",
    "\n",
    "# Optimizasyon\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "# Kayıp fonksiyonu\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Eğitim döngüsü\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (real_data, labels) in enumerate(dataloader):\n",
    "        batch_size = real_data.size(0)\n",
    "\n",
    "        # Discriminator için gerçek ve sahte etiketler\n",
    "        real_labels = torch.ones(batch_size, len(emotions))\n",
    "        fake_labels = torch.zeros(batch_size, len(emotions))\n",
    "\n",
    "        # Discriminator'ı eğitme\n",
    "        optimizer_d.zero_grad()\n",
    "        real_data = real_data.float()\n",
    "        real_output = discriminator(real_data)\n",
    "        real_loss = criterion(real_output, real_labels)\n",
    "        real_loss.backward()\n",
    "\n",
    "        noise = torch.randn(batch_size, input_size)\n",
    "        fake_data = generator(noise)\n",
    "        fake_output = discriminator(fake_data.detach())\n",
    "        fake_loss = criterion(fake_output, fake_labels)\n",
    "        fake_loss.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # Generator'ı eğitme\n",
    "        optimizer_g.zero_grad()\n",
    "        fake_output = discriminator(fake_data)\n",
    "        generator_loss = criterion(fake_output, real_labels)\n",
    "        generator_loss.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], D Loss: {real_loss.item()+fake_loss.item()}, G Loss: {generator_loss.item()}')\n",
    "\n",
    "# Üretilmiş cümleleri kaydetme\n",
    "generated_reviews = []\n",
    "for i in range(1000):  # 1000 tane cümle üretelim\n",
    "    noise = torch.randn(1, input_size)\n",
    "    generated_review = generator(noise)\n",
    "    generated_reviews.append(generated_review)\n",
    "\n",
    "generated_df = pd.DataFrame({'review': generated_reviews})\n",
    "generated_df.to_csv(\"emotion_generated_reviews.csv\", index=False)\n",
    "print(generated_df.head())\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\"\"\"\n",
    "# Modeli yükleyin\n",
    "generator = torch.load('emotion_generator.pth')\n",
    "discriminator = torch.load('emotion_discriminator.pth')\n",
    "\n",
    "# Üretim için fonksiyon\n",
    "def generate_sentences(num_sentences=1000):\n",
    "    generator.eval()\n",
    "    sentences = []\n",
    "    for _ in range(num_sentences):\n",
    "        noise = torch.randn(1, input_size)\n",
    "        generated_sentence = generator(noise).detach().numpy()\n",
    "        sentences.append(generated_sentence)\n",
    "    return sentences\n",
    "\n",
    "# Cümleleri oluşturun\n",
    "generated_sentences = generate_sentences()\n",
    "\n",
    "# Discriminator kullanarak duygusal tonları belirleme\n",
    "def classify_emotion(sentences):\n",
    "    discriminator.eval()\n",
    "    emotions = ['happy', 'angry', 'sad', 'neutral']\n",
    "    classified_emotions = []\n",
    "    for sentence in sentences:\n",
    "        sentence_tensor = torch.tensor(sentence).float()\n",
    "        output = discriminator(sentence_tensor)\n",
    "        emotion = emotions[torch.argmax(output)]\n",
    "        classified_emotions.append(emotion)\n",
    "    return classified_emotions\n",
    "\n",
    "# Cümlelerin duygusal tonlarını belirleyin\n",
    "emotional_tones = classify_emotion(generated_sentences)\n",
    "\n",
    "# Sonuçları kaydedin\n",
    "results_df = pd.DataFrame({'sentence': generated_sentences, 'emotion': emotional_tones})\n",
    "results_df.to_csv(\"classified_emotional_sentences.csv\", index=False)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Veri yükleme\n",
    "data = pd.read_csv(\"classified_emotional_sentences.csv\")\n",
    "\n",
    "# Metin ve etiketleri ayırma\n",
    "texts = data['review'].values\n",
    "labels = data['sentiment'].values\n",
    "\n",
    "# Etiketleri sayısal hale getirme\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Veriyi eğitim ve test setlerine ayırma\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenizer ve dizilere dönüştürme\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=100)\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=100)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Modeli oluşturma\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(input_dim=10000, output_dim=128, input_length=100))\n",
    "model_lstm.add(LSTM(128, return_sequences=True))\n",
    "model_lstm.add(LSTM(64))\n",
    "model_lstm.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Modeli derleme\n",
    "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Modeli eğitme\n",
    "history = model_lstm.fit(X_train_padded, y_train, epochs=5, batch_size=64, validation_split=0.1)\n",
    "\n",
    "# Test seti üzerinde değerlendirme\n",
    "loss, accuracy = model_lstm.evaluate(X_test_padded, y_test)\n",
    "print(f'LSTM Model Test Accuracy: {accuracy:.4f}')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# GAN ile üretilmiş veri yükleme\n",
    "gan_data = pd.read_csv(\"classified_emotional_sentences.csv\")\n",
    "\n",
    "# Metin ve etiketleri ayırma\n",
    "gan_texts = gan_data['sentence'].values\n",
    "gan_labels = gan_data['emotion'].values\n",
    "\n",
    "# Etiketleri sayısal hale getirme\n",
    "gan_encoded_labels = label_encoder.transform(gan_labels)\n",
    "\n",
    "# Veriyi eğitim ve test setlerine ayırma\n",
    "X_gan_train, X_gan_test, y_gan_train, y_gan_test = train_test_split(gan_texts, gan_encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenizer ve dizilere dönüştürme\n",
    "X_gan_train_sequences = tokenizer.texts_to_sequences(X_gan_train)\n",
    "X_gan_test_sequences = tokenizer.texts_to_sequences(X_gan_test)\n",
    "\n",
    "X_gan_train_padded = pad_sequences(X_gan_train_sequences, maxlen=100)\n",
    "X_gan_test_padded = pad_sequences(X_gan_test_sequences, maxlen=100)\n",
    "\n",
    "# Modeli eğitme\n",
    "history_gan = model_lstm.fit(X_gan_train_padded, y_gan_train, epochs=5, batch_size=64, validation_split=0.1)\n",
    "\n",
    "# Test seti üzerinde değerlendirme\n",
    "gan_loss, gan_accuracy = model_lstm.evaluate(X_gan_test_padded, y_gan_test)\n",
    "print(f'LSTM Model GAN Üretilmiş Veriler ile Test Accuracy: {gan_accuracy:.4f}')\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
